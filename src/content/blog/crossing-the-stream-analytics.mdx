---
title: 'Crossing the stream analytics...'
date: '2017-03-15'
excerpt: ''
tags: ['Uncategorized']
readingTime: 5
legacy: true
originalUrl: 'https://simonlambcodesblog.wordpress.com/2017/03/15/crossing-the-stream-analytics/'
---

# Crossing the stream analytics...

As part of a hackathon last week I had the pleasure of playing with some technologies that I didn’t have an enormous amount of exposure to. The focus of my team's part of the hackathon was the following:

1.  Analysing a video with Cognitive Services
2.  Processing the results from Cognitive Services
3.  Saving the results to a data store so it can be displayed
4.  Visualizing the results with Power BI Embedded

My personal work and this resulting post deals mainly with the second and third items as there were some unique challenges that came into play when utilising the output that came from streaming analytics. The flow of data looked a little something like this: ![Crossing The Stream Analytics](/images/blog/crossing-the-stream-analytics-1758894622694.png) We hacked on Azure Media Services being analysed by Azure Cognitive Services and posted its results to Event Hub. We then used Stream Analytics to process and group the data before throwing the results into a Service Bus queue. We grabbed the data with a simple Logic App, inserted it into a SQL Azure table and rounded out the flow with a visualisation in Power BI Embedded.
I know that’s a lot of ingredients to throw in an omelette but let’s go break a few eggs… First, the results from the Cognitive API services were sent to Event Hub. This was processed with a streaming analytics job that looked like this: ![Crossing The Stream Analytics](/images/blog/crossing-the-stream-analytics-1758894623448.png) Cognitive Services was used analyse the stream and the emotion APIs detected the happiness of the subjects in the video. We wanted to group the happiness into a rolling type window every two minutes and streaming analytics provides us with the ability to do that. Streaming analytics allows several types of temporal operations that can be performed on streaming data.
These are [Dn835055](https://msdn.microsoft.com/library/dn835055.aspx), [Dn835041](https://msdn.microsoft.com/library/dn835041.aspx) and [Dn835051](https://msdn.microsoft.com/library/dn835051.aspx) windows and I’ll explain how each works and why we chose to use the sliding window.

### Tumbling Windows

Tumbling window functions are used to segment a data stream into distinct time segments and perform a function against them, such as the example below. The key differentiators of a Tumbling window are that they repeat, do not overlap and an event cannot belong to more than one tumbling window. ![Crossing The Stream Analytics](/images/blog/crossing-the-stream-analytics-1758894624763.png)

### Hopping Windows

Hopping window functions hop forward in time by a fixed period. It may be easy to think of them as Tumbling windows that can overlap, so events can belong to more than one Hopping window result set. To make a Hopping window the same as a Tumbling window one would simply specify the hop size to be the same as the window size. ![Crossing The Stream Analytics](/images/blog/crossing-the-stream-analytics-1758894624904.png)

### Sliding Windows

Sliding window functions, unlike Tumbling or Hopping windows, produce an output **only** when an event occurs. Every window will have at least one event and the window continuously moves forward by an € (epsilon). Like Hopping Windows, events can belong to more than one Sliding Window. ![Crossing The Stream Analytics](/images/blog/crossing-the-stream-analytics-1758894625023.png) Since we wanted a rolling window only when data exists we chose to use sliding windows and the resulting query looked like this: ![Crossing The Stream Analytics](/images/blog/crossing-the-stream-analytics-1758894625194.png) The output was pushed onto a Service Bus queue and this is where things got a little interesting.
To process the resulting data on the Service Bus queue we implemented a Logic App to take the JSON, parse it and insert the results into an Azure SQL table. The Logic App was simple and these were the basic steps involved: ![Crossing The Stream Analytics](/images/blog/crossing-the-stream-analytics-1758894626457.png) After configuring the step to pick up any items from the Service Bus queue we found that the resulting data had been serialised strangely and contained extra meta data that prevented JSONConvert from being able to parse it: https://gist.github.com/slamb2k/98627d5e4d876927877bbf4536513c03 Luckily I found this great [TechNet](https://social.technet.microsoft.com/wiki/contents/articles/34750.integrating-service-bus-stack-with-logic-apps-and-azure-functions.aspx) article with some details on how to clean up the incoming data so I could insert it in database. With the help of this article I created the Azure Function below to clean up the message content so we could use it effectively in the subsequent steps. https://gist.github.com/slamb2k/44b1600264e9b892958539ccce5cc47d https://gist.
github.com/slamb2k/98627d5e4d876927877bbf4536513c03.js Note the _**#r "Newtonsoft.Json"**_ line which includes the reference to the library which contains JsonConvert. The _**#r**_ is how you add reference to an Azure Function so we can deserialise and work with the payload from the Service Bus. After I created this Azure Function I could add it to Logic App and clean up the data inline.
The Logic App then looked like this: ![Crossing The Stream Analytics](/images/blog/crossing-the-stream-analytics-1758894627952.png) Note that we had to wrap the content in another JSON element as the Azure Function expects valid JSON to be passed to it. We could then finally insert the data into the table in our Azure SQL database.
![Crossing The Stream Analytics](/images/blog/crossing-the-stream-analytics-1758894628883.png) Unfortunately, the **_AVG_** value from our stream analytics output isn't available in the designer so we have to go to the code view and update the HappinessScore by adding the _**"@{body('CleanGarbage').avg}"**_ function explicitally. It will then reflect in the designer view: https://gist.github.
com/slamb2k/d7f4b0085c67e273c3077741d57cf500 With the data in the database we could then spin up our Power BI Embedded dashboard and view the Happiness trend over time. ![Crossing The Stream Analytics](/images/blog/crossing-the-stream-analytics-1758894629749.png) While the scenario being discussed here was specific I hope that helps some other people out there when they want to process the same kind of information as I did. If I make just a few other people's life easier it would have been worth it...
